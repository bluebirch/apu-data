# Inledning

Detta är en ny analys av de data som samlades in från Arbetsförmedlingens testcenter 2012--2016. Den tidigare analysen redovisades i rapporten *Testdata från APU på Arbetsförmedlingen 2012--2016* av Marcus Lindskog, Nicklas Timstedt och Maria Öhrstedt.

## Datainsamling

Från rapporten *Testdata från APU*:

> Förfrågan om att samla in data gick ut till samtliga testcentrum på Arbetsförmedlingen under senare delen av 2016. Resultat från tester som inte är datoriserade har testcentrumen inte anmodats att skicka in, då det arbetet skulle blivit för omfattande. Detta gäller exempelvis DLS och Rey Complex Figure Test.
>
> Data från enstaka testcentrum har inte kommit med i underlaget. Vissa testcentrum hade också problem att exportera WTS data. Testdata från äldre versioner av WTS har inte fungerat att exportera. Efter några försök med alternativa instruktioner för export som inte heller fungerade beslöt Enheten Strategi att vi redan hade tillräckligt med testdata. Inskickade data har sammanfogats och rensats från ovidkommande uppgifter av Johannes Bengtsson, Ar Enheten Sdh, syd. [@LindskogTimstedtOhrstedt2017 3]

## Databearbetning och analys

### Tidigare analyser

I rapporten *Testdata från APU* användes statistikverktyget R:

> Data har importerats till Excel och analys har sedan utförts i statistikprogrammet R. Fredrik Jansson Dahlén och Petra Ornstein på Enheten Forskning och Utveckling, Analysavdelningen, har varit behjälplig i arbetet med att ta fram statistiskt underlag. [@LindskogTimstedtOhrstedt2017 3]

Någon närmare information om vilka analyser som gjordes, hur databearbetning, rensning och liknande har jag inte. Inte heller har jag frågat efter de R-script som rimligen måste finnas kvar på Analysavdelningen. Jag har helt enkelt valt att börja om från början.

### Mina analyser

Min ingång härär att all databearbetning och alla analyser skall vara dokumenterade och sårbara. All "städning" av rådata skall vara dokumenterad och reproducerbar. Därför har jag utgått från det rådata i form av Excel-filer som jag fått tillgång till och gör varje steg i bearbetningen med hjälp av det som förr i tiden benämndes ADB, *automatisk databehandling*. Bearbetningen har gått till i följande steg:

1. Excelfilerna konverterades till CSV (*comma-separated values*) med hjälp av ett litet program i Python (så fick jag samtidigt en anledning att öva mig i Python-programmering). CSV-filer har den fördelen att de är lätta att läsa in i såväl R som Excel.
2. De enskilda CSV-filerna kombinerades till en sammanslagen CSV-fil med data från alla tre testsystem (VTS, ATS och HTS). Här gjorde jag stora ansträngningar att få till läsbara variabelnamn anpassade till R-nomenklatur. Excel-datum har också konverterats till vanliga datum. Under bearbetningen noterade jag följande:
    1. I flera datafiler förekommer testpersonkoden TP999, ofta flera gånger. Jag vet inte vad den betyder och jag har helt enkelt ignorerat dem.
    2. Det förekommer dubletter av testpersonkoder från olika testcenter. Detta löste jag genom att förse varje testperson med ett nummer för testcentret i fråga. Detta nummer är taget från det nummer som förekommer i filnamnet i rådata. Så har dubletten av till exempel testperson *NVA1449038* i både testcentrum 14 och 15 lösts genom att de fått koderna *NVA1449038/14* respektive *NVA1449038/15*.
    3. Den sammanslagna CSV-filen har lästs in i R och dataintegritet och liknande har unersökts med hjälp av R. *Detta dokument är en redovisning av just den integritetsanalysen*.

